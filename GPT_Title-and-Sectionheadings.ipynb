{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f6044e-463f-4988-bc46-a3c3d641c15c",
   "metadata": {
    "gather": {
     "logged": 1698242209391
    }
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Network communication and web scraping\n",
    "import requests\n",
    "import urllib.request\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "# Azure AI\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient, FormRecognizerClient, DocumentModelAdministrationClient\n",
    "\n",
    "# HTML\n",
    "import html\n",
    "\n",
    "# Table formatting\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Utilities for PDF parsing\n",
    "from common.utils import parse_pdf, read_pdf_files, text_to_base64, get_search_results, model_tokens_limit, num_tokens_from_docs, num_tokens_from_string\n",
    "\n",
    "# Prompt generation\n",
    "from common.prompts import COMBINE_QUESTION_PROMPT, COMBINE_PROMPT, COMBINE_PROMPT_TEMPLATE\n",
    "\n",
    "# Environment variables and loading\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")\n",
    "\n",
    "# Directory and file management\n",
    "os.makedirs(\"data/books/\",\n",
    "            exist_ok=True)\n",
    "\n",
    "# Constants and global variables\n",
    "BLOB_CONTAINER_NAME = \"auflastung\"\n",
    "BASE_CONTAINER_URL = f\"https://storagegenaiasinfo.blob.core.windows.net/{BLOB_CONTAINER_NAME}/\"\n",
    "LOCAL_FOLDER = \"./data/books\"\n",
    "# Options: gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-4, or gpt-4-32k\n",
    "MODEL = \"gpt-35-turbo\"\n",
    "\n",
    "os.makedirs(LOCAL_FOLDER,\n",
    "            exist_ok=True)\n",
    "\n",
    "# Set ENV variables\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "\n",
    "\n",
    "# Display Python executable path and installed packages\n",
    "print(sys.executable)\n",
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ff0d4-56e3-4bed-843d-28c7a092069b",
   "metadata": {
    "gather": {
     "logged": 1698240996976
    }
   },
   "outputs": [],
   "source": [
    "embedder = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\",\n",
    "                            chunk_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87c647-158c-4f85-b569-5b9462f06c83",
   "metadata": {},
   "source": [
    "## 1 - Manual Document Cracking with Push to Vector-based Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a3275",
   "metadata": {
    "gather": {
     "logged": 1698241020109
    }
   },
   "outputs": [],
   "source": [
    "def list_files_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    List all PDF files in the specified folder and its subfolders.\n",
    "    \n",
    "    @param folder_path: str\n",
    "    The path to the directory where the search for PDF files starts.\n",
    "    \n",
    "    @return: list\n",
    "    A list of full paths to the PDF files found within the folder_path.\n",
    "    \"\"\"\n",
    "    pdf_file_paths = [os.path.join(root, file) for root, dirs, files in os.walk(folder_path) for file in files if file.lower().endswith(\".pdf\")]\n",
    "    return pdf_file_paths\n",
    "\n",
    "\n",
    "\n",
    "def table_to_html(table):\n",
    "    \"\"\"\n",
    "    Convert a table object to an HTML table representation.\n",
    "    \n",
    "    @param table: object\n",
    "    An object representing the table, with properties row_count, cells, whereeach cell has properties row_index, column_index, column_span, row_span, kind, and content.\n",
    "    \n",
    "    @return: str\n",
    "    A string that represents the HTML markup for the table.\n",
    "    \"\"\"\n",
    "    table_html = [\"<table>\"]\n",
    "\n",
    "    rows = [sorted([cell for cell in table.cells if cell.row_index == i], key=lambda cell: cell.column_index) for i in range(table.row_count)]\n",
    "    \n",
    "    for row_cells in rows:\n",
    "        row_html = [\"<tr>\"]\n",
    "        for cell in row_cells:\n",
    "            # Determine the cell tag type\n",
    "            tag = \"th\" if cell.kind in (\"columnHeader\", \"rowHeader\") else \"td\"\n",
    "            # Build the span attributes\n",
    "            cell_spans = f' colSpan=\"{cell.column_span}\"' if cell.column_span > 1 else \"\"\n",
    "            cell_spans += f' rowSpan=\"{cell.row_span}\"' if cell.row_span > 1 else \"\"\n",
    "            # Construct cell HTML\n",
    "            row_html.append(f\"<{tag}{cell_spans}>{html.escape(cell.content)}</{tag}>\")\n",
    "        row_html.append(\"</tr>\")\n",
    "        table_html.append(\"\".join(row_html))\n",
    "    \n",
    "    table_html.append(\"</table>\")\n",
    "    return \"\".join(table_html)\n",
    "\n",
    "\n",
    "\n",
    "file_paths_list = list_files_in_folder(LOCAL_FOLDER)\n",
    "print(len(file_paths_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643fb64-31b4-4ebe-b9e6-cb4fe8e754bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_custom_pdf(file,\n",
    "                     form_recognizer=False,\n",
    "                     formrecognizer_endpoint=None,\n",
    "                     formrecognizerkey=None,\n",
    "                     model=\"prebuilt-document\",\n",
    "                     from_url=False,\n",
    "                     verbose=False):\n",
    "    \"\"\"\n",
    "    Analyze a PDF document using Azure Form Recognizer and extract the page map with section headings and table HTML.\n",
    "\n",
    "    @param file_path: Path to the PDF file to be analyzed.\n",
    "    @param model: The model ID used for analysis. Default is \"prebuilt-document\" for prebuilt document model.\n",
    "    @param verbose: If True, prints additional information during processing.\n",
    "\n",
    "    @return: A list of tuples, each containing page number, offset, page text, and section heading for each page.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize Form Recognizer\n",
    "    credential = AzureKeyCredential(os.environ[\"FORM_RECOGNIZER_KEY\"])\n",
    "    form_recognizer_client = DocumentAnalysisClient(endpoint=os.environ[\"FORM_RECOGNIZER_ENDPOINT\"],\n",
    "                                                    credential=credential)\n",
    "    \n",
    "    # Analyze document\n",
    "    with open(file, \"rb\") as document_file:\n",
    "        poller = form_recognizer_client.begin_analyze_document(model_id=model,\n",
    "                                                               document=document_file)\n",
    "    results = poller.result()\n",
    "    \n",
    "    # Initialize variables for page mapping\n",
    "    page_map = []\n",
    "    offset = 0\n",
    "\n",
    "    # Process each page\n",
    "    for page_num, page in enumerate(results.pages):\n",
    "\n",
    "        ### Title\n",
    "        page_text = \"\"\n",
    "\n",
    "        section_heading = \" \".join(\n",
    "            paragraph.content\n",
    "            for paragraph in results.paragraphs\n",
    "            if paragraph.bounding_regions[0].page_number == page_num + 1 and paragraph.role == \"sectionHeading\"\n",
    "        )\n",
    "        \n",
    "        # Process tables on current page\n",
    "        tables_on_page = [table for table in results.tables if table.bounding_regions[0].page_number == page_num + 1]\n",
    "        page_offset = page.spans[0].offset\n",
    "        page_length = page.spans[-1].offset + page.spans[-1].length - page_offset\n",
    "        table_chars = [-1] * page_length\n",
    "\n",
    "        for table_id, table in enumerate(tables_on_page):\n",
    "            for span in table.spans:\n",
    "                for i in range(span.length):\n",
    "                    idx = span.offset - page_offset + i\n",
    "                    if 0 <= idx < page_length:\n",
    "                        table_chars[idx] = table_id\n",
    "\n",
    "        # Build page text\n",
    "        added_tables = set()\n",
    "        for idx, table_id in enumerate(table_chars):\n",
    "            if table_id == -1:\n",
    "                page_text += results.content[page_offset + idx]\n",
    "            elif table_id not in added_tables:\n",
    "                page_text += table_to_html(tables_on_page[table_id])\n",
    "                added_tables.add(table_id)\n",
    "\n",
    "        # Append page information\n",
    "        page_text += \" \"\n",
    "        page_map.append((page_num, offset, page_text, section_heading))\n",
    "        offset += len(page_text)\n",
    "\n",
    "    return page_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec51630-dccb-4fd0-8f00-20679faab2d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dictionary to map books to page maps\n",
    "book_pages_map = {}\n",
    "\n",
    "# Loop through each book in 'file_paths_list'\n",
    "for book in file_paths_list:\n",
    "    \n",
    "    # Start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Parse the PDF\n",
    "    book_map = parse_custom_pdf(file=book,\n",
    "                                model=\"prebuilt-layout\",\n",
    "                                verbose=True)\n",
    "    book_pages_map[book] = book_map\n",
    "    \n",
    "    # Elapsed time\n",
    "    end_time = time.time() - start_time\n",
    "    \n",
    "    # Print the time taken and number of pages found\n",
    "    print(f\"Parsing took: {end_time:.6f} seconds\\n{book} contained {len(book_map)} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f9b7d-99e6-426d-a47e-343c7e8b492e",
   "metadata": {},
   "source": [
    "## Create Vector-based index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d46e7c5-49c4-40f3-bb2d-79a9afeab4b1",
   "metadata": {
    "gather": {
     "logged": 1698241996403
    }
   },
   "outputs": [],
   "source": [
    "book_index_name = \"custom-auflastung-use-headings\" #choose index name, only small letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b07e84b-d306-4bc9-9124-e64f252dd7b2",
   "metadata": {
    "gather": {
     "logged": 1698242226782
    }
   },
   "outputs": [],
   "source": [
    "# Create Azure Search Vector-based Index\n",
    "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
    "params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df4db6b-969b-4b91-963f-9334e17a4e3c",
   "metadata": {
    "gather": {
     "logged": 1698242231669
    }
   },
   "outputs": [],
   "source": [
    "index_payload = {\n",
    "    \"name\": book_index_name,\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": \"true\", \"filterable\": \"true\" },\n",
    "        {\"name\": \"title\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
    "        {\"name\": \"chunk\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
    "        {\"name\": \"chunkVector\",\"type\": \"Collection(Edm.Single)\",\"searchable\": \"true\",\"retrievable\": \"true\",\"dimensions\": 1536,\"vectorSearchConfiguration\": \"vectorConfig\"},\n",
    "        {\"name\": \"name\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "        {\"name\": \"location\", \"type\": \"Edm.String\", \"searchable\": \"false\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "        {\"name\": \"page_num\",\"type\": \"Edm.Int32\",\"searchable\": \"false\",\"retrievable\": \"true\"},\n",
    "        {\"name\": \"sectionheading\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
    "        \n",
    "    ],\n",
    "    \"vectorSearch\": {\n",
    "        \"algorithmConfigurations\": [\n",
    "            {\n",
    "                \"name\": \"vectorConfig\",\n",
    "                \"kind\": \"hnsw\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"semantic\": {\n",
    "        \"configurations\": [\n",
    "            {\n",
    "                \"name\": \"my-semantic-config\",\n",
    "                \"prioritizedFields\": {\n",
    "                    \"titleField\": {\n",
    "                        \"fieldName\": \"title\"\n",
    "                    },\n",
    "                    \"prioritizedContentFields\": [\n",
    "                        {\n",
    "                            \"fieldName\": \"chunk\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"prioritizedKeywordsFields\": [\n",
    "                        {\n",
    "                            \"fieldName\": \"sectionheading\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + book_index_name,\n",
    "                 data=json.dumps(index_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8aa55-1b60-4057-93db-0d4a89993a57",
   "metadata": {
    "gather": {
     "logged": 1698242252826
    }
   },
   "outputs": [],
   "source": [
    "for bookname,bookmap in book_pages_map.items():\n",
    "    for page in tqdm(bookmap):\n",
    "        try:\n",
    "            page_num = page[0] + 1\n",
    "            content = page[2]\n",
    "            book_url = BASE_CONTAINER_URL + bookname\n",
    "            sectionheading=page[3]\n",
    "            upload_payload = {\n",
    "                \"value\": [\n",
    "                    {\n",
    "                        \"id\": text_to_base64(bookname + str(page_num)),\n",
    "                        \"title\": f\"{bookname}_page_{str(page_num)}\",\n",
    "                        \"chunk\": content,\n",
    "                        \"chunkVector\": embedder.embed_query(content if content != \"\" else \"-------\"),\n",
    "                        \"name\": bookname,\n",
    "                        \"location\": book_url,\n",
    "                        \"page_num\": page_num,\n",
    "                        \"sectionheading\": sectionheading,\n",
    "                        \"@search.action\": \"upload\"\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + book_index_name + \"/docs/index\",\n",
    "                              data=json.dumps(upload_payload),\n",
    "                              headers=headers,\n",
    "                              params=params)\n",
    "            if r.status_code != 200:\n",
    "                print(r.status_code)\n",
    "                print(r.text)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715cddcf-af7b-4006-a047-853fc7a66be3",
   "metadata": {},
   "source": [
    "## Query the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b408798-5527-44ca-9dba-cad2ee726aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION = \"what normally rich dad do that is different from poor dad?\"\n",
    "# QUESTION = \"Tell me a summary of the book Boundaries\"\n",
    "# QUESTION = \"Dime que significa la radiacion del cuerpo negro\"\n",
    "# QUESTION = \"what is the acronym of the main point of Made to Stick book\"\n",
    "# QUESTION = \"who won the soccer worldcup in 1994?\" # this question should have no answer\n",
    "QUESTION = \"Show all the compliance references for the standart EN 45502\"#\"Show the file with document number D1330781\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b182ade-0ddd-47a1-b1eb-2cbf435c317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_indexes = [book_index_name]\n",
    "\n",
    "ordered_results = get_search_results(QUESTION,\n",
    "                                     vector_indexes,\n",
    "                                     k=10,\n",
    "                                     reranker_threshold=1,\n",
    "                                     vector_search=True,\n",
    "                                     similarity_k=2,\n",
    "                                     query_vector=embedder.embed_query(QUESTION))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd2f3f2-2d66-4bd4-b90b-d30970b71af4",
   "metadata": {},
   "source": [
    "**Note**: that we are picking a larger k=10 since these chunks are NOT of 5000 chars each like prior notebooks, but instead each page is a chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ff796-dab1-4817-a3a5-82eeff6c0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETION_TOKENS = 1000\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL,\n",
    "                      temperature=0.5,\n",
    "                      max_tokens=COMPLETION_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744aba20-b3fd-4286-8d58-2ddfccc77734",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_docs = []\n",
    "for key,value in ordered_results.items():\n",
    "    location = value[\"location\"] if value[\"location\"] is not None else \"\"\n",
    "    top_docs.append(Document(page_content=value[\"chunk\"], metadata={\"source\": location+os.environ['BLOB_SAS_TOKEN']}))\n",
    "        \n",
    "print(f\"Number of chunks: {len(top_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c4d56-8c2d-47d6-8717-810f156f1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of tokens of our docs\n",
    "if(len(top_docs) > 0):\n",
    "    tokens_limit = model_tokens_limit(MODEL) # this is a custom function we created in common/utils.py\n",
    "    prompt_tokens = num_tokens_from_string(COMBINE_PROMPT_TEMPLATE) # this is a custom function we created in common/utils.py\n",
    "    context_tokens = num_tokens_from_docs(top_docs) # this is a custom function we created in common/utils.py\n",
    "    \n",
    "    requested_tokens = prompt_tokens + context_tokens + COMPLETION_TOKENS\n",
    "    \n",
    "    chain_type = \"map_reduce\" if requested_tokens > 0.9 * tokens_limit else \"stuff\"  \n",
    "    \n",
    "    print(f\"\"\"System prompt token count: {prompt_tokens}\n",
    "    Max Completion Token count: {COMPLETION_TOKENS}\n",
    "    Combined docs (context) token count: {context_tokens}\n",
    "    --------\n",
    "    Requested token count: {requested_tokens}\n",
    "    Token limit for {MODEL}: {tokens_limit}\n",
    "    Chain Type selected: {chain_type}\"\"\")\n",
    "        \n",
    "else:\n",
    "    print(\"NO RESULTS FROM AZURE SEARCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf3a3f-2b4d-4806-8b92-eb982c52b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chain_type == \"stuff\":\n",
    "    chain = load_qa_with_sources_chain(llm,\n",
    "                                       chain_type=chain_type, \n",
    "                                       prompt=COMBINE_PROMPT)\n",
    "elif chain_type == \"map_reduce\":\n",
    "    chain = load_qa_with_sources_chain(llm,\n",
    "                                       chain_type=chain_type, \n",
    "                                       question_prompt=COMBINE_QUESTION_PROMPT,\n",
    "                                       combine_prompt=COMBINE_PROMPT,\n",
    "                                       return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b412c56-650f-4ca4-a868-9954f83679fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Try with other language as well\n",
    "response = chain({\"input_documents\": top_docs,\n",
    "                  \"question\": QUESTION,\n",
    "                  \"language\": \"English\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f07b08-87bd-4518-b2f2-03ee1096f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response['output_text']))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
